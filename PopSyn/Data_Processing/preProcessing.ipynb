{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfadf32-445a-41c3-816e-c945e677d348",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This Notebook processes the Data from Danmarks Statistics and DTU's TU Data set.\n",
    "We have 5 different data sets that are cleaned, categorised and eventually merged into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c02a7f-6b73-4c1e-bf92-1fa1d9f2afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic module import\n",
    "import os           # Working directory\n",
    "import pandas as pd # Data processing\n",
    "import numpy as np  # Scientific computing/matrix algebra\n",
    "import matplotlib.pyplot as plt # Common graphing interface (check also plotly and plotnine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Support functions\n",
    "\n",
    "def count_nan_values(dataframe):\n",
    "    nan_count_df = pd.DataFrame([(column, dataframe[column].isna().sum()) for column in dataframe.columns], columns=['Column', 'NaN Count'])\n",
    "    nan_count_df = nan_count_df.loc[(nan_count_df['NaN Count'] > 0)].sort_values('NaN Count')\n",
    "\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        print(nan_count_df.reset_index(inplace=False, drop=True))\n",
    "\n",
    "\n",
    "def process_data_frame(file_name, categorical_cols = [], numerical_cols_float = [], numerical_cols_int = [], character_cols =[]):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_name, sep=',', engine='python')\n",
    "\n",
    "    # Set categorical columns\n",
    "    df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "    # Set numerical columns of float type\n",
    "    df[numerical_cols_float] = df[numerical_cols_float].replace(\n",
    "        np.nan, -1).astype('float64')\n",
    "\n",
    "    # Set numerical columns of integer type\n",
    "    df[numerical_cols_int] = df[numerical_cols_int].replace(\n",
    "        np.nan, -1).astype('int64')\n",
    "\n",
    "    # Set character columns\n",
    "    df[character_cols] = df[character_cols].astype('category')\n",
    "\n",
    "    # Drop unnamed columns\n",
    "    df.drop(df.columns[df.columns.str.contains(\n",
    "        'unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e57e2-b152-4143-a39d-73f894a5453e",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "1. job.csv\n",
    "2. pop.csv\n",
    "3. commuter_codes.csv\n",
    "4. commuter_values.csv\n",
    "5. session.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c52c56-7592-4cea-aba5-5c467214e4f8",
   "metadata": {},
   "source": [
    "#### Define data location\n",
    "\n",
    "Due to the fact that we are handleing sensible data, we need to ensure that any data stays on DTU's servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce89179",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c1ef2-446c-4ae9-aee0-d943fb1e53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ~/snap/snapd-desktop-integration/83/Documents/Thesis/data\n",
    "\n",
    "os.chdir('/Users/luis/Desktop/Data_extracted/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84150e01-8106-4c0d-ba84-88ece1f075cf",
   "metadata": {},
   "source": [
    "### Job Data\n",
    "\n",
    "- Load 'job.csv'\n",
    "- Set attribute types to numerical and categorical values\n",
    "- extract *job municipalities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d4ed9-baee-4de8-bd9f-0655ffd84468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 'categorical' and 'numerical' lists and changing the datatypes accordingly\n",
    "job_categorical = ['Municipality', 'AgeGroup', 'Gender', 'Sector', 'Socio']\n",
    "# job_float = []\n",
    "job_int = ['Val', 'Year']\n",
    "# job_character = []\n",
    "\n",
    "job_df = process_data_frame(\n",
    "    'job.csv', categorical_cols=job_categorical, numerical_cols_int=job_int)\n",
    "\n",
    "# Renaming categories from Maend and Kvinder (Man and Women) to 1 and 2.\n",
    "job_df['Gender'] = job_df.Gender.cat.rename_categories({'Men': '1', 'Women': '2'})\n",
    "\n",
    "\n",
    "# Saving this set for further investigation\n",
    "job_mun = set(job_df['Municipality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89339da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a48673-a581-4e75-89bf-80e979792373",
   "metadata": {},
   "source": [
    "### Commuter Data\n",
    "\n",
    "- load 'commuter_codes.csv' and 'commuter_values.csv'\n",
    "- Set attribute types to numerical and categorical values\n",
    "- extract *workplace names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3947da-e445-4408-a0b7-ef691be93925",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_categorical = ['Gender', 'Residence', 'Work']\n",
    "# cm_float = []\n",
    "cm_int = ['Val', 'Year'] \n",
    "# cm_character = []\n",
    "\n",
    "cm_df = process_data_frame('commuter_codes.csv', categorical_cols = cm_categorical, numerical_cols_int = cm_int, )\n",
    "cm_df_val = process_data_frame('commuter_values.csv',  categorical_cols = cm_categorical, numerical_cols_int = cm_int, )\n",
    "\n",
    "\n",
    "# Renaming categories from Maend and Kvinder (Man and Women) to 1 and 2.\n",
    "cm_df['Gender'] = cm_df.Gender.cat.rename_categories({'M': '1', 'K': '2'})\n",
    "cm_df_val['Gender'] = cm_df_val.Gender.cat.rename_categories({'Men': '1', 'Women': '2'})\n",
    "\n",
    "\n",
    "### Merge the data frames and create commuter database\n",
    "\n",
    "# Standard merge is based on set index.\n",
    "cm_df_tot = pd.merge(cm_df, cm_df_val, left_index=True,\n",
    "                     right_index=True, suffixes=('_c', '_v'))\n",
    "\n",
    "# Convert the residence code for further work\n",
    "cm_df_tot['Residence_c'] = cm_df_tot['Residence_c'].astype('int64')\n",
    "cm_df_tot['Work_c'] = cm_df_tot['Work_c'].astype('int64')\n",
    "\n",
    "# Create workplace and residence code sets\n",
    "workplace_codes = cm_df_tot[['Work_c', 'Work_v']].drop_duplicates()\n",
    "residence_codes = cm_df_tot[['Residence_c', 'Residence_v']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df_tot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cb707-79f6-4ca8-92f4-ab518ffe3aaa",
   "metadata": {},
   "source": [
    "### Population Data\n",
    "- load 'pop.csv'\n",
    "- Set attribute types to numerical and categorical values\n",
    "- extract job municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2609a-2ad4-40c9-a9d6-4656a89b0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_categorical = ['Municipality', 'PopSocio', 'Sector', 'AgeGroup', 'Gender', 'edu']\n",
    "# pop_float = []\n",
    "pop_int = ['Year', 'Val']\n",
    "# pop_character = []\n",
    "\n",
    "pop_df = process_data_frame(\n",
    "    'pop.csv', categorical_cols = pop_categorical, numerical_cols_int = pop_int)\n",
    "\n",
    "# Renaming categories from Maend and Kvinder (Man and Women) to 1 and 2.\n",
    "pop_df['Gender'] = pop_df.Gender.cat.rename_categories({'Men': '1', 'Women': '2'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d79d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pop_df = pd.merge(pop_df, workplace_codes, left_on='Municipality', right_on='Work_v')\n",
    "pop_df[['edu_c', 'edu_v']] = pd.DataFrame(pop_df['edu'].str.split(\" \", n=1, expand=True).astype('category'))\n",
    "pop_df['PopSocio_c'] = pop_df.PopSocio.cat.rename_categories({'Enrolled in education': '0', 'Employed': '1', 'Unemployed':'2', 'Outside the labour force':'3'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf04a3-1711-486c-9224-22f6c11f9bc0",
   "metadata": {},
   "source": [
    "### TU Data\n",
    "- load 'session.csv', 'bil.csv' and 'household.csv'\n",
    "- Set attribute types to numerical and categorical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74391-1f87-4aa8-a16e-73ddb3e50579",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_categorical = ['DayPrimTargetMuncode', 'DayPrimTargetPurp', 'DayStartCityCode', 'DayStartJourneyRole', 'DayStartMuncode', 'DayStartPurp', \n",
    "               'DiaryDaytype', 'DiaryMonth', 'DiaryWeekday', 'Handicap', 'HomeAdrCityCode', 'HomeAdrMunCode', 'HomeParkPoss', \n",
    "               'HousehAccomodation', 'HousehAccOwnorRent', 'HwDaysReason', 'InterviewType', 'JstartMuncode', 'JstartNUTS', \n",
    "               'JstartType', 'ModeChainTypeDay', 'NuclFamType', 'PosInFamily', 'PrimModeDay', 'PrimOccMuncode', 'RespEdulevel', \n",
    "               'RespHasBicycle', 'ResphasDrivlic', 'RespHasRejsekort', 'RespHasSeasonticket', 'RespIsmemCarshare', 'RespNotripReason', \n",
    "               'RespPrimOcc', 'RespSex', 'SduMuncode', 'WorkHourType', 'WorkParkPoss', 'WorkPubPriv','DayStartFareZone', 'DayStartGMMzone', 'HomeAdrFareZone', \n",
    "               'HomeAdrGMMzone', 'JstartFareZone', 'JstartGMMzone', 'PrimOccFareZone', 'PrimOccGMMzone', 'SduGMMzone', 'HwDayspW', 'DayJourneyType']\n",
    "\n",
    "session_float = ['DayNumJourneys', 'GISdistHW', 'HomeAdrDistNearestStation', 'JstartDistNearestStation', 'SessionWeight', 'TotalBicLen', \n",
    "                   'TotalFuelConsumpMJ', 'TotalGramCO2', 'TotalGramCO2eq', 'TotalLenExclComTrans', 'WeightOver6']\n",
    "\n",
    "session_int = ['DiaryDate', 'DiaryYear', 'FamNumAdults', 'FamNumDrivLic', 'FamNumPers', 'FamNumPers1084', 'FamNumPersO6', \n",
    "                 'HomeAdrCitySize', 'HousehCarOwnership', 'HousehNumAdults', 'HousehNumcars', 'HousehNumDrivLic', 'HousehNumPers', \n",
    "                 'HousehNumPers1084', 'HousehNumPersO6',  'IncFamily', 'IncFamily2000', 'IncHouseh', 'IncHouseh2000', \n",
    "                 'IncNuclFamily', 'IncNuclFamily2000', 'IncRespondent', 'IncRespondent2000', 'IncSpouse', 'IncSpouse2000', 'kmarbud', \n",
    "                 'NightsAway', 'NuclFamNumAdults', 'NuclFamNumDrivLic', 'NuclFamNumPers', 'NuclFamNumPers1084', 'NuclFamNumPersO6', \n",
    "                 'NumTripsCorr', 'NumTripsExclComTrans', 'RespAgeCorrect', 'RespAgeSimple', 'RespDrivlicYear', 'RespYearBorn', \n",
    "                 'SessionId', 'TotalLen', 'TotalMin', 'TotalMinExclComTrans', 'TotalMotorLen', 'TotalMotorMin', 'TotalNumTrips', \n",
    "                 'WorkatHomeDayspM', 'WorkHoursPw']\n",
    "\n",
    "session_characters = ['DayStartNUTS', 'HomeAdrNearestStation', 'HomeAdrNUTS', 'JstartNearestStation', 'PrimOccNUTS', 'PseudoYear', 'SduNUTS']\n",
    "\n",
    "\n",
    "session_df = process_data_frame('session.csv', session_categorical, session_float, session_int, session_characters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4eebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mean = session_df.DiaryYear.value_counts().mean()   \n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29683d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_categorical = ['FuelType', 'NplateColour','CarOwnershipType']\n",
    "# bil_float = []\n",
    "bil_int = ['SessionId', 'bilnr', 'CarModelYear']\n",
    "# bil_character = []\n",
    "\n",
    "\n",
    "bil_df = process_data_frame('bil.csv', bil_categorical, numerical_cols_int = bil_int)\n",
    "\n",
    "\n",
    "def groupby_latest_model_year(x):\n",
    "    latest_indices = x['CarModelYear'].idxmax()  # Find the index with the latest CarModelYear\n",
    "    latest_rows = x.loc[latest_indices]  # Get the rows corresponding to the latest indices\n",
    "    return tuple(latest_rows[col] for col in ['CarModelYear', 'FuelType'])\n",
    "\n",
    "\n",
    "\n",
    "# Group by the non-unique IDs and apply the sampling function\n",
    "bil_df_sampled = bil_df.groupby('SessionId').apply(groupby_latest_model_year).apply(pd.Series)\n",
    "bil_df_sampled.columns = ['CarModelYear', 'FuelType']\n",
    "\n",
    "session_df = pd.merge(session_df, bil_df_sampled, on='SessionId', how='left')\n",
    "session_df['CarModelYear'] = session_df['CarModelYear'].replace(np.nan, -1).astype('int64')\n",
    "session_df['FuelType'] = session_df['FuelType'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af366b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "household_categorical = ['Relation', 'PosInFamily', 'Gender', 'HasDrivLic']\n",
    "# household_float = []\n",
    "household_int = ['SessionId', 'medlnr', 'YearBorn', 'AgeSimple']\n",
    "# household_character = []\n",
    "\n",
    "household_df = process_data_frame('household.csv', categorical_cols = household_categorical,  numerical_cols_int = household_int)\n",
    "\n",
    "# Count people between 4 and 15\n",
    "count_4_to_15 = household_df.query('0 <= AgeSimple <= 15').groupby('SessionId').size().reset_index(name='KidsBetween0and15')\n",
    "\n",
    "# Count people between 0 and 4\n",
    "count_0_to_4 = household_df.query('0 <= AgeSimple <= 4').groupby('SessionId').size().reset_index(name='KidsBetween0and4')\n",
    "\n",
    "# Merge counts and fill NaN values with 0\n",
    "household_df_children = count_4_to_15.merge(count_0_to_4, on='SessionId', how='outer').fillna(0)\n",
    "\n",
    "session_df = pd.merge(session_df, household_df_children,on='SessionId', how='left')\n",
    "session_df[['KidsBetween0and15', 'KidsBetween0and4']] = session_df[['KidsBetween0and15', 'KidsBetween0and4']].replace(np.nan, 0).astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_categorical = ['OrigNUTS', 'DestNUTS']\n",
    "tur_float = ['TurId', 'TurNr', 'TripCount', 'DepartHH', 'DepartMM', 'DepartMSM', \n",
    "              'ArrivalHH', 'ArrivalMM', 'ArrivalMSM', 'DestDwelTime', 'OrigMuncode', 'OrigCityCode', \n",
    "              'OrigGMMzone', 'OrigFareZone', 'OrigDistNearestStation',  'DestCityCode', \n",
    "              'DestFareZone', 'DestDistNearestStation', 'OrigPurp', 'DestPurp', 'DestEscortPurp', \n",
    "              'ShopAmount', 'TripPurp', 'TripPurpGroup', 'SimplWorktour', 'SimplWorkNumstop', 'GISdist', \n",
    "              'NumModes', 'SumLen', 'SumMin', 'SumMotorLen', 'SumMotorMin', 'SumMJ', 'SumCO2', 'SumCO2eq', \n",
    "              'ModeChainType', 'PrimMode', 'PrimModeDrivPass', 'SecMode', 'PrimModeSumlen', 'SecModeSumlen', \n",
    "              'FirstMode', 'LastMode', 'PartyorAlone', 'PartyNumu10', 'PartyNum1017', 'PartyNumAdults', \n",
    "              'BicType', 'CarPassDriver', 'CarPassContext', 'CarCostShare', 'CarUsageCarNo', 'PtTicketType', \n",
    "              'PtPrice', 'PtBicType', 'PtPrimMode', 'PtNumBoardings', 'PtAccTime', 'PtFirstWaitTime', \n",
    "              'PtInvTime', 'PtChangeAndWaitTime', 'PtEgrTime', 'PtAccMode', 'PtEgrMode', 'PtAccLen', 'PtEgrLen', \n",
    "              'TrainMode', 'TrainAccMode', 'TrainEgrMode', 'TrainAccMin', 'TrainEgrMin', 'TrainAccLen', \n",
    "              'TrainEgrLen', 'TrainAccDist', 'TrainEgrDist', 'JourneyId', 'JourneyRole', 'GISdistJourneyStartP']\n",
    "tur_int = ['SessionId', 'DestGMMzone', 'DestMuncode',]\n",
    "tur_character = ['OrigNearestStation','DestNearestStation', 'FirstStation', 'LastStation']\n",
    "\n",
    "tur_df = process_data_frame('tur.csv', tur_categorical, tur_float, tur_int, tur_character)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge Car and Household data to TU Data\n",
    "session_df[['SessionId', 'HousehNumcars', 'HousehCarOwnership', 'CarModelYear', 'FuelType', 'IncFamily2000', 'IncRespondent2000', 'FamNumPers', 'FamNumAdults', 'FamNumPers1084','FamNumPersO6','KidsBetween0and15','KidsBetween0and4']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e5dca-f284-4a52-b5ea-fb787cbc8e97",
   "metadata": {},
   "source": [
    "## Denmark Satatistics Data cleansing\n",
    "\n",
    "- cleaning Municipality codes due to aggregation and total values.\n",
    "- [Regions values can be dropped - Codes 082-085]\n",
    "- [Provincesvalues can be dropped - Codes 1-11]\n",
    "- [Outside Denmark can be dropped - Code 950]\n",
    "- [All Denmark can be dropped - Code ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe060f84-d62d-4efb-90d5-c17d5ce19be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Job Municipalities: ',job_df.Municipality.nunique())\n",
    "print('PoP Municipalities: ',pop_df.Municipality.nunique())\n",
    "print('Commuter Municipalities: ',cm_df_tot.Work_v.nunique())\n",
    "print('TU Workplaces: ', session_df.PrimOccMuncode.nunique())\n",
    "print('TU HomeAddress: ', session_df.HomeAdrMunCode.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c75d9-8870-4876-b4e9-baa635e080da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to be dropped for each category\n",
    "regions_codes = ['82', '83', '84', '85']\n",
    "provinces_codes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
    "outside_denmark_code = '950'\n",
    "all_denmark_code = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f55b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_cat = set(list(session_df.PrimOccMuncode.cat.categories))\n",
    "hom_cat = set(list(session_df.HomeAdrMunCode.cat.categories))\n",
    "\n",
    "print(f\"Number of unique values in 'PrimOccMuncode': {occ_cat}\")\n",
    "print(f\"Number of unique values in 'HomeAdrMunCode': {hom_cat}\")\n",
    "\n",
    "# Calculate and print differences between 'Work_c' and 'PrimOccMuncode' columns\n",
    "\n",
    "diff_work = list(set(workplace_codes.Work_c) ^ occ_cat)\n",
    "diff_work = [int(i) for i in diff_work]\n",
    "print(\"Differences between 'Work_c' and 'PrimOccMunCode' columns:\")\n",
    "print([int(i) for i in diff_work])\n",
    "\n",
    "work = workplace_codes.loc[~workplace_codes['Work_c'].isin(diff_work)]\n",
    "\n",
    "\n",
    "# Calculate and print differences between 'Residence_c' and 'HomeAdrMunCode' columns\n",
    "\n",
    "diff_residence = list(set(residence_codes.Residence_c) ^ hom_cat)\n",
    "diff_residence = [int(i) for i in diff_residence]\n",
    "print(\"Differences between 'Residence_c' and 'HomeAdrMunCode' columns:\")\n",
    "print([int(i) for i in diff_residence])\n",
    "\n",
    "residence = residence_codes.loc[~residence_codes['Residence_c'].isin(diff_residence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a8c8d-370d-4c0b-9432-e810fed7e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = session_df.drop(session_df[session_df.PrimOccMuncode.isin(diff_work) | session_df.HomeAdrMunCode.isin(diff_residence)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c179b-d69a-4fcf-b7d9-1f38025b6ad9",
   "metadata": {},
   "source": [
    "## Check column values to prepare merge with pop, job and commute data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fbff6-151d-45e5-99d2-ecd9efed6bb5",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200796d-7460-4a41-8c5a-5a01271be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_drop_rows(df, column_name, condition_value):\n",
    "    # Store the count of rows before the drop operation\n",
    "    rows_before = len(df)\n",
    "\n",
    "    # Drop rows based on the condition\n",
    "    df.drop(df[df[column_name] == condition_value].index, inplace = True)\n",
    "\n",
    "    # Calculate the count of rows after the drop operation\n",
    "    rows_after = len(df)\n",
    "\n",
    "    # Calculate how many rows have been dropped\n",
    "    rows_dropped = rows_before - rows_after\n",
    "\n",
    "    # Print the number of rows dropped\n",
    "    print(f\"{rows_dropped} rows have been dropped.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d6792-819e-40e5-8c8c-5b1249e11e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_column_info(df, exclude_columns=None):\n",
    "    for column in df.columns:\n",
    "        if exclude_columns and column in exclude_columns:\n",
    "            continue\n",
    "\n",
    "        unique_values = sorted(df[column].unique().tolist())\n",
    "        num_unique = len(unique_values)\n",
    "        \n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        print(f\"Number of Unique Values: {num_unique}\")\n",
    "        print(f\"Unique Values: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1a61a-cfb2-47c3-961c-4b27d63f9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unused_categories(df, column_name):\n",
    "    # Get the current categories\n",
    "    current_categories = set(df[column_name].cat.categories)\n",
    "\n",
    "    # Remove unused categories from the specified categorical column\n",
    "    df[column_name].cat.remove_unused_categories()\n",
    "\n",
    "    # Get the removed categories\n",
    "    removed_categories = current_categories - set(df[column_name].cat.categories)\n",
    "\n",
    "    # Print the removed categories and their count\n",
    "    for category in removed_categories:\n",
    "        print(f\"Removed category '{category}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcc365-1ae2-49cb-bc9e-e0e66bdbe57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_categories(df, column_name, new_categories):\n",
    "    for new_category in new_categories:\n",
    "        if new_category not in df[column_name].cat.categories:\n",
    "            df[column_name] = df[column_name].cat.add_categories(new_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3effefc-e19e-456f-9840-da408590fca0",
   "metadata": {},
   "source": [
    "### Pop Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e9c04-ded5-4cdd-90d1-ccfe49810e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_info(pop_df, 'Val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38015400-bcb1-4597-9383-5ef48764e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'edu' column, convert them to a list\n",
    "unique_edu_values = set(pop_df['edu'].unique().tolist())\n",
    "\n",
    "\n",
    "pop_df['edu_c'] = pop_df['edu_c'].cat.add_categories('H99')\n",
    "pop_df['edu_v']=pop_df['edu_v'].cat.add_categories('Long-term further education')\n",
    "pop_df['edu']=pop_df['edu'].cat.add_categories('H99 Long-term further education')\n",
    "\n",
    "# Update values in the 'edu_c' column where 'H70' or 'H80' is replaced with 'H99'\n",
    "pop_df.loc[(pop_df['edu_c'] == 'H70') | (pop_df['edu_c'] == 'H80'), 'edu_c'] == 'H99'\n",
    "pop_df.loc[(pop_df['edu'] == 'H70 Masters programs') | (pop_df['edu'] == 'H80 PhD programs'), 'edu'] = 'H99 Long-term further education'\n",
    "pop_df.loc[(pop_df['edu_v'] == 'Masters programs') | (pop_df['edu_v'] == 'PhD programs'), 'edu_v'] = 'Long-term further education'\n",
    "\n",
    "# Count and drop rows where 'edu' is 'H90 Not stated'\n",
    "pop_df = count_and_drop_rows(pop_df, 'edu', 'H90 Not stated')\n",
    "\n",
    "# Set variables as categories\n",
    "pop_df[['PopSocio_c', 'edu', 'edu_c', 'edu_v']] = pop_df[['PopSocio_c', 'edu', 'edu_c', 'edu_v']].astype('category')\n",
    "\n",
    "# Renaming categories from Maend and Kvinder (Man and Women) to 1 and 2.\n",
    "pop_df['Gender'] = pop_df['Gender'].cat.rename_categories({'Men': '1', 'Women': '2'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e21b65-6e93-4990-80f7-f1d9fe957cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155882-8785-4a00-bc1a-0389c0b77fcc",
   "metadata": {},
   "source": [
    "### Job Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2032574-34c3-4af7-92b7-0cb9b16a6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_column_info(job_df, 'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd6b8-e1a1-4123-808a-cae25d539d4c",
   "metadata": {},
   "source": [
    "### Commuter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86428-c169-4d1b-b52a-d77cce966c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_column_info(cm_df_tot, ['value_c', 'value_v'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c1b51-f69e-432d-99a5-5e4e2920b0e3",
   "metadata": {},
   "source": [
    "### TU Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a7e2c-358e-45c8-a632-e8e0b97eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are droppping the NaN values in the following variables ['RespSex', 'RespEdulevel', 'RespPrimOcc', 'HomeAdrMunCode'].\n",
    "### The purpose is to have a clean dataset base line for the analysis. The set of values represents the common ground for all the datasets.\n",
    "\n",
    "# List of variables to check for NaN values\n",
    "check_var = ['RespSex', 'RespEdulevel', 'RespPrimOcc', 'HomeAdrMunCode']\n",
    "\n",
    "# Iterate through each variable\n",
    "for var in check_var:\n",
    "    print(f\"\\nProcessing variable: {var}\")\n",
    "\n",
    "    # Unique values before removing missing values\n",
    "    unique_before = session_df[var].unique()\n",
    "    count_before = len(session_df)\n",
    "\n",
    "    # Drop rows with missing values in the current variable\n",
    "    session_df.drop(session_df[session_df[var].isnull()].index, inplace=True)\n",
    "    print(f\"Number of dropped rows: {count_before - len(session_df)}\")\n",
    "\n",
    "    # Unique values after removing missing values\n",
    "    unique_after = session_df[var].unique()\n",
    "\n",
    "    # If unique values have decreased, print the unique values\n",
    "    if len(unique_before) >= len(unique_after):\n",
    "        print(f\"Unique values of '{var}' after removing missing values:\")\n",
    "        print(f\"Number of Values \",len(session_df[var].unique()), session_df[var].unique().sort_values().tolist(), )\n",
    "        print(f\"Values dropped: {set(unique_before) ^ set(unique_after)}\")\n",
    "    else:\n",
    "        print(\"no values dropped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7288c-0e2f-403c-bf2a-e2ad8d830202",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Defining Age Groups\n",
    "###\n",
    "\n",
    "# Define age bins and corresponding categories\n",
    "age_bins = [-1, 15, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 200]\n",
    "age_cats = ['under 15 years', '15-19 years', '20-24 years', '25-29 years', '30-34 years', '35-39 years',\n",
    "            '40-44 years', '45-49 years', '50-54 years', '55-59 years', '60-64 years', '65-69 years', 'over 69 years']\n",
    "\n",
    "# Create a categorical variable 'AgeGroup' based on 'RespAgeCorrect' using the specified bins and labels\n",
    "session_df['AgeGroup'] = pd.cut(session_df.RespAgeCorrect, age_bins, labels=age_cats)\n",
    "\n",
    "###\n",
    "### Modifying the Education variable\n",
    "###\n",
    "\n",
    "session_df['Education'] = ''\n",
    "session_df.loc[session_df['RespEdulevel'].isin([1., 2., 3., 4.]), 'Education'] = 'H10' ### 'H10 Primary education','H90 Not stated'\n",
    "session_df.loc[session_df['RespEdulevel'] == 5., 'Education'] = 'H20' ### 'H20 Upper secondary education',\n",
    "session_df.loc[session_df['RespEdulevel'] == 6., 'Education'] = 'H50' ### 'H50 Vocational bachelors educations',\n",
    "session_df.loc[session_df['RespEdulevel'] == 9., 'Education'] = 'H35' ### 'H35 Qualifying educational programs',\n",
    "session_df.loc[session_df['RespEdulevel'] == 11., 'Education'] = 'H30' ### 'H30 Vocational Education and Training (VET)',\n",
    "session_df.loc[session_df['RespEdulevel'] == 12., 'Education'] = 'H40' ### 'H40 Short cycle higher education',\n",
    "session_df.loc[session_df['RespEdulevel'] == 13., 'Education'] = 'H60' ### 'H60 Bachelors programs',\n",
    "session_df.loc[session_df['RespEdulevel'] == 14., 'Education'] = 'H99' ### 'H70 Masters programs', 'H80 PhD programs',\n",
    "\n",
    "session_df['Education'] = session_df['Education'].astype('category')\n",
    "\n",
    "###\n",
    "### Modifying the Population Socio variable\n",
    "###\n",
    "session_df['PopSocio'] = ''\n",
    "\n",
    "# Define mappings for occupation codes to PopSocio values\n",
    "occupation_mappings = {\n",
    "    '0': [103., 107., 116., 120., 130.], ### 'Enrolled in education': '0',\n",
    "    '1': [211., 210., 221., 231., 232., 233., 222.], ### 'Employed': '1',\n",
    "    '2': [310.], ### 'Unemployed':'2',\n",
    "    '3': [320., 390., 360., 370., 350.], ### 'Outside the labour force':'3'\n",
    "}\n",
    "\n",
    "# Use a loop to update 'PopSocio' based on occupation codes\n",
    "for pop_socio, occupation_codes in occupation_mappings.items():\n",
    "    session_df.loc[session_df['RespPrimOcc'].isin(occupation_codes), 'PopSocio'] = pop_socio\n",
    "\n",
    "session_df['PopSocio'] = session_df['PopSocio'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6086927-d1e1-4e69-9377-0ff090730992",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c12e1-60fc-4d74-91eb-b1dbfbb9efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the dataframes to be merged\n",
    "\n",
    "### Pop dataframe\n",
    "\n",
    "pop_df_merge = pop_df[['Year', 'Gender', 'AgeGroup', 'Sector', 'Val', 'Work_c', 'edu_c', 'PopSocio_c']].copy()\n",
    "pop_rename = {\n",
    "    'Work_c':'MunicipalityOrigin',\n",
    "    'edu_c':'Education',\n",
    "    'PopSocio_c':'PopSocio',\n",
    "}\n",
    "\n",
    "pop_df_merge.rename(index=str, columns=pop_rename, inplace=True)\n",
    "\n",
    "\n",
    "### Session dataframe\n",
    "\n",
    "session_df_merge = session_df.copy()\n",
    "\n",
    "session_rename = {\n",
    "    'PrimOccMuncode': 'MunicipalityDest',\n",
    "    'HomeAdrMunCode': 'MunicipalityOrigin',\n",
    "    'RespSex': 'Gender',\n",
    "    'DiaryYear': 'Year',\n",
    "}\n",
    "session_df_merge.rename(index=str, columns=session_rename,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee44db-6708-405c-8ed4-b619d46c3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge the dataframes\n",
    "\n",
    "# Set common columns used for indexing and merging\n",
    "idx_list = ['MunicipalityOrigin', 'Education', 'PopSocio', 'AgeGroup', 'Gender', 'Year']\n",
    "\n",
    "pop_df_merge = pop_df_merge.groupby(idx_list + ['Sector'], as_index=False, observed=True).sum()\n",
    "\n",
    "### Calculate the sums and percentages of each group\n",
    "\n",
    "# Calculate the sums for each group and create a 'sums' column\n",
    "pop_df_merge['sums'] = pop_df_merge.groupby(idx_list, as_index=False, observed=True)['Val'].transform('sum')\n",
    "\n",
    "# Calculate the percentage and create a 'percent' column\n",
    "pop_df_merge['percent'] = pop_df_merge.Val/pop_df_merge.sums\n",
    "\n",
    "\n",
    "# Convert selected columns to strings and categories\n",
    "for var in ['MunicipalityOrigin', 'Education', 'PopSocio', 'AgeGroup', 'Gender']:\n",
    "    pop_df_merge[var] = pop_df_merge[var].astype(str)\n",
    "    pop_df_merge[var] = pop_df_merge[var].astype('category')\n",
    "    session_df_merge[var] = session_df_merge[var].astype(str)\n",
    "    session_df_merge[var] = session_df_merge[var].astype('category')\n",
    "\n",
    "### Filter and set indices for both DataFrames\n",
    "# Drop rows with years outside the range 2009-2021 - due to missing data in the population dataset (Denmark Statistics)\n",
    "session_df_merge_cond = session_df_merge[(session_df_merge['Year'] > 2008) & (session_df_merge['Year'] < 2022)].set_index(idx_list)\n",
    "pop_df_merge_cond = pop_df_merge.copy().set_index(idx_list)\n",
    "\n",
    "# Merge the DataFrames using an inner join\n",
    "merged = session_df_merge_cond.join(pop_df_merge_cond, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df_merge_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad87ec9-b614-4347-8cb5-2b6f53bdcaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample the Sector variable based on the percentages of each group\n",
    "\n",
    "# Keep only the columns needed for sampling\n",
    "merged = merged[['SessionId', 'Sector', 'percent']].reset_index(drop=True)\n",
    "\n",
    "# Define a sampling function for aggregation\n",
    "def groupby_sample(x):\n",
    "    if all(x.percent == 0.):  # Check if all percent values are 0\n",
    "        return np.random.choice(x.Sector)\n",
    "    else:\n",
    "        return np.random.choice(x.Sector, p=x.percent)\n",
    "\n",
    "\n",
    "# Apply the sampling function to each group\n",
    "samp_df = merged.groupby('SessionId', as_index=False).apply(groupby_sample).reset_index()\n",
    "samp_df = samp_df.drop(['index'], axis=1)\n",
    "samp_df.columns = ['SessionId', 'Sector']\n",
    "\n",
    "# Merge the sampled DataFrame with the original DataFrame\n",
    "session_samp_df = samp_df.merge(session_df_merge_cond.reset_index(),on='SessionId', how='inner')\n",
    "\n",
    "under16 = session_df_merge_cond.loc[session_df_merge_cond['RespAgeCorrect'] < 16]\n",
    "over69 = session_df_merge_cond.loc[session_df_merge_cond['RespAgeCorrect'] > 69]\n",
    "\n",
    "excluded_ageGroups = pd.concat([under16, over69]).reset_index()\n",
    "session_allAges_df = pd.concat([excluded_ageGroups, session_samp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_allAges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = pd.read_csv('OTM_Zones_SesID_new.csv', sep=',')\n",
    "zones.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_Zones_df = session_allAges_df.merge(zones, on='SessionId', how='left')\n",
    "\n",
    "gmmZone_list = ['DayStartGMMzone', 'HomeAdrGMMzone',\n",
    "                'JstartGMMzone', 'PrimOccGMMzone', 'SduGMMzone']\n",
    "session_Zones_df[gmmZone_list] = session_Zones_df[gmmZone_list].astype('float64')\n",
    "\n",
    "mapping_columns = {'DayStart_OTM': 'DayStartGMMzone', 'Homeadr_OTM': 'HomeAdrGMMzone',\n",
    "                   'JStart_OTM': 'JstartGMMzone', 'PrimOcc_OTM': 'PrimOccGMMzone', 'SDU_OTM': 'SduGMMzone'}\n",
    "\n",
    "for index, row in session_Zones_df.iterrows():\n",
    "    for otm_col, gmm_col in mapping_columns.items():\n",
    "        if not pd.isnull(row[otm_col]):\n",
    "            session_Zones_df.at[index, gmm_col] = row[otm_col]\n",
    "\n",
    "session_Zones_df[gmmZone_list] = session_Zones_df[gmmZone_list].astype('category')\n",
    "session_Zones_df.rename(columns={'DayStartGMMzone': 'DayStartZone', 'HomeAdrGMMzone': 'HomeAdrZone',\n",
    "                         'JstartGMMzone': 'JstartZone', 'PrimOccGMMzone': 'PrimOccZone', 'SduGMMzone': 'SduZone'}, inplace=True)\n",
    "session_Zones_df.drop(['DayStart_OTM', 'Homeadr_OTM', 'JStart_OTM',\n",
    "              'PrimOcc_OTM', 'SDU_OTM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d760b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = session_Zones_df.replace(-1, np.nan)\n",
    "total_df.describe()._append(samp_df.isnull().sum().rename('isnull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbaf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_values = ['Year', 'SessionId', 'Gender', 'RespAgeCorrect', 'RespEdulevel', 'AgeGroup', 'Education', 'Handicap', 'PopSocio', 'RespPrimOcc', 'Sector',\n",
    "                     'FamNumAdults', 'FamNumPers', 'KidsBetween0and15', 'KidsBetween0and4',\n",
    "                     'HousehNumAdults', 'HousehNumPers', 'HousehNumcars', 'HousehCarOwnership', 'CarModelYear', 'FuelType',\n",
    "                     'IncRespondent2000', 'IncFamily2000', 'IncHouseh2000', 'IncSpouse2000',\n",
    "                     'MunicipalityOrigin', 'MunicipalityDest', 'HomeAdrZone', 'PrimOccZone',\n",
    "                     'HwDayspW', 'WorkHoursPw', 'WorkHourType',\n",
    "                     'RespHasBicycle', 'ResphasDrivlic', 'RespHasRejsekort', 'RespIsmemCarshare', \n",
    "\n",
    "                     'HomeParkPoss', 'RespHasSeasonticket', 'HousehAccomodation', 'HousehAccOwnorRent', \n",
    "                     'PosInFamily', 'PrimModeDay','ModeChainTypeDay']\n",
    "                    \n",
    "\n",
    "sim_df = total_df[simulation_values].copy()\n",
    "\n",
    "# These do not make sense, since they have attributes that are only specific for employees\n",
    "# 'WorkPubPriv', 'WorkParkPoss',\n",
    "\n",
    "# These variable have to specific attribvutes that do not fit fot the simulation model\n",
    "# 'NuclFamType',\n",
    "\n",
    "# These day variables are not relevant\n",
    "# 'DayJourneyType', 'DayPrimTargetPurp', 'DayStartJourneyRole', 'DayStartPurp',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd76e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans_categories(df, column_name):\n",
    "    # Copy the DataFrame\n",
    "    filtered_df = df[['SessionId', 'MunicipalityOrigin', 'Education',\n",
    "                      'PopSocio', 'AgeGroup', 'Gender', 'Year', column_name]].copy()\n",
    "\n",
    "    # Set indices for both DataFrames\n",
    "    idx = ['MunicipalityOrigin', 'Education','PopSocio', 'AgeGroup', 'Gender', 'Year']\n",
    "\n",
    "    # Calculate group sizes\n",
    "    group_df = filtered_df.copy().drop(['SessionId'], axis=1)\n",
    "    group_df['counter'] = 1\n",
    "    group_df = group_df.groupby(by=['MunicipalityOrigin', 'Education', 'PopSocio', 'AgeGroup', 'Gender', 'Year', column_name],\n",
    "                                      as_index=False, dropna=True).sum()\n",
    "    group_df['sums'] = group_df.groupby(['MunicipalityOrigin', 'Education', 'PopSocio', 'AgeGroup',\n",
    "                                        'Gender', 'Year'], as_index=False, observed=True)['counter'].transform('sum')\n",
    "\n",
    "    group_df['sums'] = group_df['sums'].replace(0, 1)\n",
    "    group_df['Percentage'] = (group_df.counter / group_df.sums)\n",
    "    group_df.drop(['counter', 'sums'], axis=1, inplace=True)\n",
    "\n",
    "    # Set indices for both DataFrames\n",
    "\n",
    "    nan_df = filtered_df.loc[filtered_df[column_name].isna()==True]\n",
    "\n",
    "    nan_df = nan_df.drop(columns=[column_name])\n",
    "    group_df_c = group_df.set_index(idx)\n",
    "    nan_df_c = nan_df.set_index(idx)\n",
    "\n",
    "    # Join DataFrames\n",
    "    matched = nan_df_c.join(group_df_c, how='inner')\n",
    "\n",
    "    # Define a sampling function for aggregation\n",
    "    def groupby_sample(x):\n",
    "        \n",
    "        if all(x.Percentage == 0.):  # Check if all percent values are 0\n",
    "            return np.random.choice(x[column_name])\n",
    "        else:\n",
    "            return np.random.choice(x[column_name], p=x.Percentage)\n",
    "\n",
    "    # Apply the sampling function\n",
    "    matched = matched.groupby('SessionId', as_index=False).apply(groupby_sample).reset_index()\n",
    "    matched = matched.drop(['index'], axis=1)\n",
    "    matched.columns = ['SessionId', column_name]\n",
    "\n",
    "    # Merge the results back to the original DataFrame\n",
    "    df = df.merge(matched, on='SessionId', how='left')\n",
    "    df[column_name] = df[column_name + '_x'].fillna(df[column_name + '_y'])\n",
    "    df = df.drop([column_name + '_x', column_name + '_y'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af837ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans_numerical(df, grouping_vars = ['MunicipalityOrigin', 'Education', 'PopSocio', 'AgeGroup','Gender']):\n",
    "\n",
    "    categories_list = df.select_dtypes(include='category').columns.tolist()\n",
    "    numericals_list = df.select_dtypes(exclude='category').columns[df.select_dtypes(exclude='category').isna().any()].tolist()\n",
    "\n",
    "\n",
    "    # Divide the dataset\n",
    "    df_auxiliar = df.drop(columns = numericals_list) # Dataframe which we don't modify\n",
    "    df_missing  = df[grouping_vars + numericals_list + ['SessionId']] # Dataframe with missings\n",
    "\n",
    "    # NaN replacement\n",
    "    f = lambda x: x.fillna(np.random.choice(x)) # Function to fill the na values using a random element \n",
    "    df_missing = df_missing.groupby(grouping_vars).transform(f) # Applying the function to the grouping\n",
    "    \n",
    "    return df_missing.merge(df_auxiliar, on='SessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df, column, pop_socio_categories=['0', '1', '2', '3']):\n",
    "    df[column] = df[column].cat.add_categories(['MISSING'])\n",
    "    condition = (df['PopSocio'].isin(pop_socio_categories)) & (df[column].isna())\n",
    "    df.loc[condition, column] = 'MISSING'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2da41",
   "metadata": {},
   "source": [
    "### Replace NaNs in categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace nans in RespHasRejsekort\n",
    "#sim_df[['SessionId','PopSocio', 'RespHasRejsekort']].loc[sim_df['RespHasRejsekort'].isna() == True]\n",
    "sim_df = replace_nans_categories(sim_df, 'RespHasRejsekort')\n",
    "\n",
    "### Replace Nans in RespIsmemCarshar\n",
    "# sim_df[['SessionId','PopSocio', 'RespIsmemCarshare']].loc[sim_df['RespIsmemCarshare'].isna() == True]\n",
    "sim_df = replace_nans_categories(sim_df, 'RespIsmemCarshare')\n",
    "\n",
    "### Replace Nans in Sector\n",
    "# sim_df[['SessionId','PopSocio', 'Sector']].loc[sim_df['Sector'].isna() == True]\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['2', '3'])) & (sim_df['Sector'].isna()), 'Sector'] = 'Activity not stated'\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['0'])) & (sim_df['Sector'].isna()), 'Sector'] = 'Education'\n",
    "sim_df = replace_nans_categories(sim_df, 'Sector')\n",
    "\n",
    "### Replace Nans in HwDayspW\n",
    "# sim_df[['SessionId','PopSocio', 'HwDayspW']].loc[sim_df['HwDayspW'].isna() == False]\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['0', '2', '3'])) &(sim_df['HwDayspW'].isna()), 'HwDayspW'] = 0\n",
    "sim_df = replace_nans_categories(sim_df, 'HwDayspW')\n",
    "\n",
    "### Replace Nans in WorkHourType\n",
    "# sim_df[['SessionId','PopSocio', 'WorkHourType']].loc[sim_df['WorkHourType'].isna() == False]\n",
    "sim_df['WorkHourType'] = sim_df['WorkHourType'].cat.add_categories(['MISSING'])\n",
    "sim_df['WorkHourType'] = sim_df['WorkHourType'].cat.add_categories(['Not working'])\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['0', '2', '3'])) & (sim_df['WorkHourType'].isna()), 'WorkHourType'] = 'Not working'\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['1'])) & (sim_df['WorkHourType'].isna()), 'WorkHourType'] = 'MISSING'\n",
    "\n",
    "### Replace Nans in FuelType\n",
    "# sim_df[['SessionId','PopSocio', 'FuelType']].loc[(sim_df['FuelType'].isna() == True) & (sim_df['HousehNumcars'] >0)]\n",
    "# fueltype_to_sample = sim_df.loc[(sim_df['FuelType'].isna()) & (sim_df['HousehNumcars'] > 0)]\n",
    "# sim_df.loc[fueltype_to_sample.index, 'FuelType'] = np.random.choice(sim_df['FuelType'].dropna().unique(), size=len(fueltype_to_sample))\n",
    "sim_df['FuelType'] = sim_df['FuelType'].cat.add_categories(['NoCar'])\n",
    "sim_df.loc[(sim_df['FuelType'].isna()) & (sim_df['HousehNumcars'] == 0), 'FuelType'] = 'NoCar'\n",
    "sim_df = replace_nans_categories(sim_df, 'FuelType')\n",
    "\n",
    "### Replace Nans in CarModelYear\n",
    "carmodelyear_to_sample = sim_df.loc[(sim_df['CarModelYear'].isna()) & (sim_df['HousehNumcars'] > 0)]\n",
    "sim_df.loc[carmodelyear_to_sample.index, 'CarModelYear'] = np.random.choice(sim_df['CarModelYear'].dropna().unique(), size=len(carmodelyear_to_sample))\n",
    "sim_df.loc[(sim_df['CarModelYear'].isna()) & (sim_df['HousehNumcars'] == 0), 'CarModelYear'] = -1\n",
    "sim_df['CarModelYear'] = sim_df['CarModelYear'].astype('category')\n",
    "\n",
    "### Replace Nans in MunicipalityDest\n",
    "# Replacing Nans with 'Missing' based on the PopSocio category\n",
    "columns_to_process = ['MunicipalityDest','PrimOccZone', 'HomeAdrZone']  # Add all columns to process\n",
    "\n",
    "for col in columns_to_process:\n",
    "    sim_df = fill_missing_values(sim_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcedaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat = ['HomeParkPoss', 'RespHasSeasonticket', 'HousehAccomodation', 'HousehAccOwnorRent', 'PosInFamily', 'PrimModeDay','ModeChainTypeDay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_cat:\n",
    "    print(i)\n",
    "    sim_df = replace_nans_categories(sim_df, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053859c",
   "metadata": {},
   "source": [
    "### Replace NaNs in Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace Nans in WorkHoursPw\n",
    "# sim_df[['PopSocio','WorkHoursPw','RespAgeCorrect']].loc[(sim_df['WorkHoursPw'].isna()==True) & (sim_df['PopSocio']!='1')]\n",
    "sim_df.loc[(sim_df['PopSocio'].isin(['0', '2', '3'])) & (sim_df['WorkHoursPw'].isna()), 'WorkHoursPw'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f032b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df_sel = df[['SessionId', 'MunicipalityOrigin', 'AgeGroup', 'Gender', 'PopSocio', 'IncRespondent2000', 'IncFamily2000']].copy()\n",
    "\n",
    "    df_filtered = df_sel.loc[(df_sel['IncRespondent2000'].notna()) & (df_sel['IncFamily2000'].notna())]\n",
    "    df_nan = df_sel[df_sel['IncRespondent2000'].isna() & df_sel['IncFamily2000'].isna()]\n",
    "\n",
    "    return df_filtered, df_nan\n",
    "\n",
    "\n",
    "def replace_nan_values_4index(df_filtered, df_nan):\n",
    "    inc_idx = ['MunicipalityOrigin', 'PopSocio', 'AgeGroup', 'Gender']\n",
    "    inc_group = df_nan.groupby(inc_idx, as_index=False).count()\n",
    "\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    for row in inc_group.iterrows():\n",
    "        group_notnan = df_filtered.loc[(df_filtered['MunicipalityOrigin'] == row[1]['MunicipalityOrigin']) & (\n",
    "            df_filtered['PopSocio'] == row[1]['PopSocio']) & (df_filtered['AgeGroup'] == row[1]['AgeGroup']) & (df_filtered['Gender'] == row[1]['Gender'])]\n",
    "\n",
    "        group_nan = df_nan.loc[(df_nan['MunicipalityOrigin'] == row[1]['MunicipalityOrigin']) & (\n",
    "            df_nan['PopSocio'] == row[1]['PopSocio']) & (df_nan['AgeGroup'] == row[1]['AgeGroup']) & (df_nan['Gender'] == row[1]['Gender'])]\n",
    "\n",
    "        if len(group_notnan) != 0:\n",
    "            replacement_values = group_notnan[['IncRespondent2000', 'IncFamily2000']].sample(\n",
    "                n=len(group_nan), replace=True, ignore_index=True).values\n",
    "\n",
    "            group_nan['IncRespondent2000'] = replacement_values[:, 0]\n",
    "            group_nan['IncFamily2000'] = replacement_values[:, 1]\n",
    "\n",
    "        return_df = pd.concat([return_df, group_nan])\n",
    "\n",
    "    return return_df\n",
    "\n",
    "def replace_nan_values_3index(df_filtered, df_nan):\n",
    "    inc_idx = ['PopSocio', 'AgeGroup', 'Gender']\n",
    "    inc_group = df_nan.groupby(inc_idx, as_index=False).count()\n",
    "\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    for row in inc_group.iterrows():\n",
    "        group_notnan = df_filtered.loc[(df_filtered['PopSocio'] == row[1]['PopSocio']) & (df_filtered['AgeGroup'] == row[1]['AgeGroup']) & (df_filtered['Gender'] == row[1]['Gender'])]\n",
    "\n",
    "        group_nan = df_nan.loc[(df_nan['PopSocio'] == row[1]['PopSocio']) & (df_nan['AgeGroup'] == row[1]['AgeGroup']) & (df_nan['Gender'] == row[1]['Gender'])]\n",
    "\n",
    "        if len(group_notnan) != 0:\n",
    "            replacement_values = group_notnan[['IncRespondent2000', 'IncFamily2000']].sample(\n",
    "                n=len(group_nan), replace=True, ignore_index=True).values\n",
    "\n",
    "            group_nan['IncRespondent2000'] = replacement_values[:, 0]\n",
    "            group_nan['IncFamily2000'] = replacement_values[:, 1]\n",
    "\n",
    "        return_df = pd.concat([return_df, group_nan])\n",
    "\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def rename_merge_columns(df, replaced_nan_df):\n",
    "    replaced_nan_df.rename(columns={'IncRespondent2000': 'IncomePerson', 'IncFamily2000': 'IncomeFam'}, inplace=True)\n",
    "    replaced_nan_df = replaced_nan_df[['SessionId', 'IncomePerson', 'IncomeFam']].reset_index()\n",
    "\n",
    "    renamed_df = pd.merge(df, replaced_nan_df, on='SessionId', how='left')\n",
    "    return renamed_df\n",
    "\n",
    "\n",
    "def fill_drop_columns(df):\n",
    "    df['IncRespondent2000'].fillna(df['IncomePerson'], inplace=True)\n",
    "    df['IncFamily2000'].fillna(df['IncomeFam'], inplace=True)\n",
    "    df.drop(columns=['IncomeFam', 'IncomePerson','index'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c10c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace Nans in IncRespondent2000 and IncFamily2000 based on dependent logic\n",
    "\n",
    "df_notnan, df_nan = prepare_data(sim_df)\n",
    "replaced_nan_df = replace_nan_values_4index(df_notnan, df_nan)\n",
    "sim_df_sample_incFam = rename_merge_columns(sim_df, replaced_nan_df)\n",
    "sim_df_sample_incFam= fill_drop_columns(sim_df_sample_incFam)\n",
    "\n",
    "sim_df_sample_incFam.loc[(sim_df_sample_incFam['IncSpouse2000'].isna() == False) & \n",
    "               (sim_df_sample_incFam['IncFamily2000'].isna()) & \n",
    "               (sim_df_sample_incFam['IncRespondent2000'].isna() == False), 'IncFamily2000'] = sim_df_sample_incFam.IncRespondent2000 + sim_df_sample_incFam.IncSpouse2000\n",
    "\n",
    "sim_df_sample_incFam.loc[(sim_df_sample_incFam['IncFamily2000'].isna()) & \n",
    "               (sim_df_sample_incFam['IncRespondent2000'].isna() == False) &\n",
    "               (sim_df_sample_incFam['IncSpouse2000'].isna()), 'IncFamily2000'] = sim_df_sample_incFam.IncRespondent2000\n",
    "\n",
    "df_notnan, df_nan = prepare_data(sim_df_sample_incFam)\n",
    "replaced_nan_df = replace_nan_values_3index(df_notnan, df_nan)\n",
    "sim_df_sample_incResp = rename_merge_columns(sim_df_sample_incFam, replaced_nan_df)\n",
    "sim_df_sample_incResp= fill_drop_columns(sim_df_sample_incResp)\n",
    "\n",
    "sim_df_sample_incResp.loc[(sim_df_sample_incResp['IncSpouse2000'].isna() == False) & \n",
    "               (sim_df_sample_incResp['IncRespondent2000'].isna()) & \n",
    "               (sim_df_sample_incResp['IncFamily2000'].isna() == False), 'IncRespondent2000'] = sim_df_sample_incResp.IncFamily2000 - sim_df_sample_incResp.IncSpouse2000\n",
    "\n",
    "sim_df_sample_incResp.loc[(sim_df_sample_incResp['IncRespondent2000'].isna()) & \n",
    "               (sim_df_sample_incResp['IncFamily2000'].isna() == False) &\n",
    "               (sim_df_sample_incResp['IncSpouse2000'].isna()), 'IncRespondent2000'] = sim_df_sample_incResp.IncFamily2000\n",
    "               \n",
    "sim_df_processed =  sim_df_sample_incResp.drop(columns=['IncSpouse2000','IncHouseh2000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_processed = replace_nans_numerical(sim_df_processed)\n",
    "sim_df_processed = replace_nans_numerical(sim_df_processed, grouping_vars = ['MunicipalityOrigin', 'Education', 'PopSocio'])\n",
    "sim_df_processed = replace_nans_numerical(sim_df_processed, grouping_vars = ['MunicipalityOrigin', 'PopSocio'])\n",
    "sim_df_processed = replace_nans_numerical(sim_df_processed, grouping_vars = ['PopSocio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(sim_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb1aa5",
   "metadata": {},
   "source": [
    "### Finnaly drop all rows that still include an NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sim_df_processed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b099efa",
   "metadata": {},
   "source": [
    "#### Check for Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3e71b",
   "metadata": {},
   "source": [
    "### Safe Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56704f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop columns with high correlation\n",
    "\n",
    "final_df.drop(columns=['RespEdulevel', 'AgeGroup'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96199d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/s212574/snap/snapd-desktop-integration/83/Documents/Thesis/MSc_PopSyn/Sigga_Luis/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('simulationData_withNewCat.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86da05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = pd.read_csv('simulationData_withNewCat.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.columns.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
