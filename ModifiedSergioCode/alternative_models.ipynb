{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNGAN (WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine import *\n",
    "#from keras.legacy import interfaces\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils.generic_utils import func_dump\n",
    "from keras.utils.generic_utils import func_load\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.utils.generic_utils import has_arg\n",
    "from keras.utils import conv_utils\n",
    "#from keras.legacy import interfaces\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Conv3D, Conv2DTranspose, Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "# Spectral Normalization layer code from: https://github.com/IShengFang/SpectralNormalizationKeras/blob/master/SpectralNormalizationKeras.py\n",
    "class DenseSN(Dense):\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                                 initializer=initializers.RandomNormal(0, 1),\n",
    "                                 name='sn',\n",
    "                                 trainable=False)\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                 W_bar = K.reshape(W_bar, W_shape)  \n",
    "        output = K.dot(inputs, W_bar)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(): # Training based on https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "    \n",
    "    def __init__(self, train, test, numerical_col_n, categorical_col_n, categories_n, categories_cum, # Data\n",
    "                     intermediate_dim_gen=256, latent_dim=100, n_hidden_layers_gen=4, # Architecture Generator\n",
    "                     intermediate_dim_dis=128, n_hidden_layers_dis=2, # Architecture Generator\n",
    "                     batch_size=64, epochs=50, validation_split=0.2, gen_learn_rate=0.001, dis_learn_rate=0.001): # Training\n",
    "        \n",
    "        # Data parameters\n",
    "        self.data_train = train\n",
    "        self.data_test = test\n",
    "        \n",
    "        self.numerical_col_n = numerical_col_n # Scalar\n",
    "        self.categorical_col_n = categorical_col_n # Scalar\n",
    "        self.categories_n = categories_n # List of scalars\n",
    "        self.categories_cum = categories_cum # List of scalars\n",
    "        \n",
    "        # Architecture parameters\n",
    "        self.input_dim_dis = train.shape[1]\n",
    "        self.latent_dim = latent_dim # Input dimension for generator\n",
    "        self.intermediate_dim_dis = intermediate_dim_dis\n",
    "        self.intermediate_dim_gen = intermediate_dim_gen\n",
    "        self.n_hidden_layers_dis = n_hidden_layers_dis\n",
    "        self.n_hidden_layers_gen = n_hidden_layers_gen\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.dis_learn_rate = dis_learn_rate\n",
    "        self.gen_learn_rate = gen_learn_rate\n",
    "        self.dis_opt = keras.optimizers.Adam(lr=self.dis_learn_rate)\n",
    "        self.gen_opt = keras.optimizers.Adam(lr=self.gen_learn_rate)\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        \n",
    "        # Sampling parameters\n",
    "        self.n_samples = test.shape[0]\n",
    "        \n",
    "        # Model creation\n",
    "        self.create_gan()\n",
    "        \n",
    "        # Session variables\n",
    "        #tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession() # Start tf session so we can run code.\n",
    "        K.set_session(sess) # Connect keras to the created session.\n",
    "        \n",
    "\n",
    "    # Generator architecture\n",
    "    def create_generator(self):\n",
    "        '''\n",
    "        This is the generator architecture, \n",
    "\n",
    "        params: \n",
    "        n_hidden_layers_gen: number of hidden layers\n",
    "        intermediate_dim_gen: value of the number of neurons for the first intermediate layer. \n",
    "            They increase in a factor of 2 (N*2). \n",
    "        latent_dim: dimension of latent space taken as input for the generator\n",
    "        \n",
    "        output:\n",
    "        The function outputs the generator model in keras. The architecture is defined by the user when\n",
    "        the whole GAN class is initialized.\n",
    "        '''\n",
    "            \n",
    "        gen_input = Input(shape=(self.latent_dim,), name='gen_input')\n",
    "        \n",
    "        # Make the generator intermediate dimensions as it should be first\n",
    "        self.intermediate_dim_gen = int(self.intermediate_dim_gen/2**(self.n_hidden_layers_gen-1))\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_gen):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(gen_input)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(intermediate)\n",
    "            self.intermediate_dim_gen *= 2 # Update the value of the number of neurons\n",
    "\n",
    "        # Final layer\n",
    "        # Categorical decode\n",
    "        x_decoded_mean_cat = [Dense(categories_n[cat], activation='softmax')(intermediate) \n",
    "                              for cat in range(len(self.categories_n))]\n",
    "\n",
    "        if numerical_col_n > 0: # If there are numerical variables, concatenate both\n",
    "            x_decoded_mean_num = Dense(self.numerical_col_n)(intermediate) # Numerical decode\n",
    "            gen_output = concatenate([x_decoded_mean_num] + x_decoded_mean_cat, name='gen_output')\n",
    "        else: # If there are no numerical variables only include the categorical output layer\n",
    "            gen_output = concatenate(x_decoded_mean_cat, name='gen_output')\n",
    "\n",
    "        return Model(inputs=gen_input, outputs=gen_output)\n",
    "    \n",
    "    # Discriminator architecture\n",
    "    def create_discriminator(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "\n",
    "        dis_input = Input(shape=(self.input_dim_dis,), name='dis_input')\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_dis):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = DenseSN(self.intermediate_dim_dis, name= 'discriminator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(dis_input)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = DenseSN(self.intermediate_dim_dis, name= 'discriminator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(intermediate)\n",
    "            self.intermediate_dim = int(self.intermediate_dim_dis/2) # Update the value of the number of neurons\n",
    "        \n",
    "        dis_output = Dense(1, activation='sigmoid', name='dis_output')(intermediate)\n",
    "        \n",
    "        return Model(dis_input, dis_output)\n",
    "    \n",
    "    # GAN creation\n",
    "    def create_gan(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.create_discriminator()\n",
    "        self.discriminator.compile(loss=self.loss, optimizer=self.dis_opt, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.create_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates observations\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated observations as input and discriminates\n",
    "        guess = self.discriminator(generated)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.gan = Model(z, guess)\n",
    "        self.gan.compile(loss=self.loss, optimizer=self.gen_opt) \n",
    "        \n",
    "\n",
    "    def gan_fit(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        # Save the generator and discriminator losses and accuracies\n",
    "        self.gen_loss = []\n",
    "        self.dis_loss = []\n",
    "        self.dis_acc = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of observations\n",
    "            idx = np.random.choice(self.data_train.shape[1], self.batch_size, replace=False)\n",
    "            obs = self.data_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_obs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            dis_loss_real = self.discriminator.train_on_batch(obs, valid)\n",
    "            dis_loss_fake = self.discriminator.train_on_batch(gen_obs, fake)\n",
    "            self.dis_loss.append(0.5 * np.add(dis_loss_real[0], dis_loss_fake[0]))\n",
    "            self.dis_acc.append(0.5 * np.add(dis_loss_real[1], dis_loss_fake[1]))\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            self.gen_loss.append(self.gan.train_on_batch(noise, valid))\n",
    "            \n",
    "            if epoch%100==0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, self.dis_loss[-1], 100*self.dis_acc[-1], self.gen_loss[-1]))\n",
    "                #print (dis_loss_real[0], dis_loss_fake[0], gen_loss)\n",
    "                #print(gen_obs[1,:], np.sum(gen_obs, axis=1))\n",
    "\n",
    "    # Sampling helper function for evaluation\n",
    "    def sampler(self):\n",
    "        z_sample = np.random.normal(0., 1.0, size=(self.n_samples, self.latent_dim))\n",
    "        prediction = self.generator.predict(z_sample).transpose()\n",
    "        samples = np.zeros((self.input_dim_dis, self.n_samples))\n",
    "        samples[:self.numerical_col_n,:]=prediction[:self.numerical_col_n,:]\n",
    "        for idx in range(len(self.categories_cum)-1):\n",
    "            idx_i = self.numerical_col_n+self.categories_cum[idx] # Initial index\n",
    "            idx_f = self.numerical_col_n+self.categories_cum[idx+1] # Final index\n",
    "            mask = np.argmax(prediction[idx_i:idx_f, :], axis=0) + idx_i\n",
    "            for n in range(self.n_samples):\n",
    "                samples[mask[n], n] = 1\n",
    "        return samples\n",
    "    \n",
    "    # GAN evaluation\n",
    "    def gan_evaluate(self, used_metric='MAE'):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        # Fit the model\n",
    "        self.gan_fit()\n",
    "        \n",
    "        # Evaluate it\n",
    "        self.samples = self.sampler()\n",
    "        self.gan_df = samples_to_df(self.samples, print_duplicates=False)\n",
    "        test_df = samples_to_df(self.data_test.transpose(), print_duplicates=False)\n",
    "\n",
    "        # Numerical bin creator \n",
    "        for var in numerical:\n",
    "            test_df[var], bins = pd.cut(test_df[var], bins=5,  retbins=True)\n",
    "            self.gan_df[var] = pd.cut(self.gan_df[var], bins=bins)\n",
    "\n",
    "        agg_vars = categorical # Variables we are using to aggregate and evaluate, change as needed \n",
    "        ##### Count creator\n",
    "        self.gan_df['count'] = 1\n",
    "        self.gan_df = self.gan_df.groupby(agg_vars, observed=True).count()\n",
    "        self.gan_df /= self.gan_df['count'].sum()\n",
    "\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby(agg_vars, observed=True).count()\n",
    "        test_df /= test_df['count'].sum()\n",
    "\n",
    "        ##### Merge and difference\n",
    "        real_and_sampled = pd.merge(test_df, self.gan_df, suffixes=['_real', '_sampled'], on=categorical, how='outer') # on= all variables\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "        real_and_sampled['diff'] = real_and_sampled.count_real-real_and_sampled.count_sampled\n",
    "        diff = np.array(real_and_sampled['diff'])\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['MAE'] = np.mean(abs(diff))\n",
    "        metrics['MSE'] = np.mean(diff**2)\n",
    "        metrics['RMSE'] = np.sqrt(np.mean(diff**2))\n",
    "        print('Evaluating with {}'.format(used_metric))\n",
    "        print('MAE:{}, MSE:{}, RMSE:{}'.format(metrics['MAE'], metrics['MSE'], metrics['RMSE']))  \n",
    "        \n",
    "        return metrics[used_metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN GP (NOT WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP\n",
    "class RandomWeightedAverage(_Merge):\n",
    "        \"\"\"\n",
    "        Provides a (random) weighted average between real and generated image samples\n",
    "        \"\"\"\n",
    "        def _merge_function(self, inputs):\n",
    "            alpha = K.random_uniform((64, 1)) # (batch_size, 1)\n",
    "            return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "        \n",
    "class WGANGP(): # Training partly based on https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "    \n",
    "    def __init__(self, train, test, numerical_col_n, categorical_col_n, categories_n, categories_cum, # Data\n",
    "                     intermediate_dim_gen=256, latent_dim=100, n_hidden_layers_gen=4, # Architecture Generator\n",
    "                     intermediate_dim_crit=128, n_hidden_layers_crit=2, # Architecture Critic\n",
    "                     batch_size=64, epochs=50, validation_split=0.2, gen_learn_rate=0.001, crit_learn_rate=0.00005, # Training\n",
    "                     lmda=10, nCritic=5): # Training\n",
    "        \n",
    "        # Data parameters\n",
    "        self.data_train = train\n",
    "        self.data_test = test\n",
    "        \n",
    "        self.numerical_col_n = numerical_col_n # Scalar\n",
    "        self.categorical_col_n = categorical_col_n # Scalar\n",
    "        self.categories_n = categories_n # List of scalars\n",
    "        self.categories_cum = categories_cum # List of scalars\n",
    "        \n",
    "        # Architecture parameters\n",
    "        self.input_dim_crit = train.shape[1]\n",
    "        self.latent_dim = latent_dim # Input dimension for generator\n",
    "        self.intermediate_dim_crit = intermediate_dim_crit\n",
    "        self.intermediate_dim_gen = intermediate_dim_gen\n",
    "        self.n_hidden_layers_crit = n_hidden_layers_crit\n",
    "        self.n_hidden_layers_gen = n_hidden_layers_gen\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.gen_learn_rate = gen_learn_rate\n",
    "        self.crit_learn_rate = crit_learn_rate\n",
    "        self.lmda = lmda # WGAN-GP parameter\n",
    "        self.nCritic = nCritic # WGAN parameter\n",
    "        self.crit_opt = keras.optimizers.RMSprop(lr=self.crit_learn_rate)\n",
    "        self.gen_opt = keras.optimizers.RMSprop(lr=self.gen_learn_rate) #keras.optimizers.Adam(lr=self.gen_learn_rate)\n",
    "        \n",
    "        # Sampling parameters\n",
    "        self.n_samples = test.shape[0]\n",
    "        \n",
    "        # Model creation\n",
    "        self.create_wgangp()\n",
    "        \n",
    "        # Session variables\n",
    "        #tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession() # Start tf session so we can run code.\n",
    "        K.set_session(sess) # Connect keras to the created session.\n",
    "        \n",
    "\n",
    "    # Generator architecture\n",
    "    def create_generator(self):\n",
    "        '''\n",
    "        This is the generator architecture, \n",
    "\n",
    "        params: \n",
    "        n_hidden_layers_gen: number of hidden layers\n",
    "        intermediate_dim_gen: value of the number of neurons for the first intermediate layer. \n",
    "            They increase in a factor of 2 (N*2). \n",
    "        latent_dim: dimension of latent space taken as input for the generator\n",
    "        \n",
    "        output:\n",
    "        The function outputs the generator model in keras. The architecture is defined by the user when\n",
    "        the whole GAN class is initialized.\n",
    "        '''\n",
    "            \n",
    "        gen_input = Input(shape=(self.latent_dim,), name='gen_input')\n",
    "        \n",
    "        # Make the generator intermediate dimensions as it should be first\n",
    "        self.intermediate_dim_gen = int(self.intermediate_dim_gen/2**(self.n_hidden_layers_gen-1))\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_gen):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform')(gen_input)\n",
    "                #intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                #intermediate = Dropout(rate=0.1)(intermediate)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform')(intermediate)\n",
    "                #intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                #intermediate = Dropout(rate=0.1)(intermediate)\n",
    "            self.intermediate_dim_gen *= 2 # Update the value of the number of neurons\n",
    "\n",
    "        # Final layer\n",
    "        # Categorical decode\n",
    "        x_decoded_mean_cat = [Dense(categories_n[cat], activation='softmax')(intermediate) \n",
    "                              for cat in range(len(self.categories_n))]\n",
    "\n",
    "        if numerical_col_n > 0: # If there are numerical variables, concatenate both\n",
    "            x_decoded_mean_num = Dense(self.numerical_col_n)(intermediate) # Numerical decode\n",
    "            gen_output = concatenate([x_decoded_mean_num] + x_decoded_mean_cat, name='gen_output')\n",
    "        else: # If there are no numerical variables only include the categorical output layer\n",
    "            gen_output = concatenate(x_decoded_mean_cat, name='gen_output')\n",
    "\n",
    "        return Model(inputs=gen_input, outputs=gen_output)\n",
    "    \n",
    "    # Critic architecture\n",
    "    def create_critic(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "\n",
    "        crit_input = Input(shape=(self.input_dim_crit,), name='crit_input')\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_crit):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim_crit, name= 'critic_hidden_{}'.format(_), kernel_initializer='he_uniform')(crit_input)\n",
    "                #intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                #intermediate = Dropout(rate=0.3)(intermediate)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim_crit, name= 'critic_hidden_{}'.format(_), kernel_initializer='he_uniform')(intermediate)\n",
    "                #intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                #intermediate = Dropout(rate=0.3)(intermediate)\n",
    "            self.intermediate_dim_crit = int(self.intermediate_dim_crit/2) # Update the value of the number of neurons\n",
    "        \n",
    "        crit_output = Dense(1, name='crit_output')(intermediate)\n",
    "        \n",
    "        return Model(crit_input, crit_output)\n",
    "    \n",
    "    \n",
    "    # GAN creation\n",
    "    def create_wgangp(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        # Create the critic and the generator\n",
    "        self.critic = self.create_critic()\n",
    "        self.generator = self.create_generator()\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Real sample input\n",
    "        real_obs = Input(shape=(self.input_dim_crit,))\n",
    "        \n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        # Generate sample based of noise (fake sample)\n",
    "        gen_obs = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake samples\n",
    "        fake = self.critic(gen_obs)\n",
    "        valid = self.critic(real_obs)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated = RandomWeightedAverage()([real_obs, gen_obs])\n",
    "        #alpha = K.random_uniform((self.batch_size, 1))\n",
    "        #interpolated = tf.keras.layers.Add()([(1 - alpha)*gen_fake, alpha*obs])\n",
    "    \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss, averaged_samples=interpolated)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic = Model(inputs=[gen_obs, z_disc], outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic.compile(loss=[self.wasserstein_loss, self.wasserstein_loss, partial_gp_loss], optimizer=self.crit_opt, loss_weights=[1, 1, self.lmda], metrics=['accuracy']) \n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        # Generate images based of noise\n",
    "        generated = self.generator(z_gen)\n",
    "        print(K.int_shape(generated))\n",
    "        # Discriminator determines validity\n",
    "        validation_on_generated = self.critic(generated)\n",
    "        # Defines generator model\n",
    "        self.generator = Model(z_gen, validation_on_generated)\n",
    "        self.generator.compile(loss=self.wasserstein_loss, optimizer=self.gen_opt)\n",
    "        \n",
    "        print(self.generatory.summary)\n",
    "        print(self.critic.summary)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    \n",
    "    def wgangp_fit(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((self.batch_size, 1))\n",
    "        fake  =  np.ones((self.batch_size, 1))\n",
    "        dummy =  np.zeros((self.batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        \n",
    "        # Loss and accuracy lists for graphing purposes\n",
    "        self.gen_loss  = []\n",
    "        self.crit_loss = []\n",
    "        self.crit_acc  = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Critic\n",
    "            # ---------------------\n",
    "\n",
    "            for _ in range(self.nCritic):\n",
    "                # Select a random batch of observations\n",
    "                idx = np.random.choice(self.data_train.shape[0], self.batch_size, replace=False) #self.data_train.shape[0]\n",
    "                obs = self.data_train[idx]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_obs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                crit_loss = self.critic.train_on_batch([obs, valid], [gen_obs, fake, dummy])\n",
    "                \n",
    "                # Clip the weights\n",
    "                #for l in self.critic.layers:\n",
    "                #    weights = l.get_weights()\n",
    "                #    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                #    l.set_weights(weights)\n",
    "                    \n",
    "            self.crit_loss.append(crit_loss[0])\n",
    "            self.crit_acc.append(crit_loss[1])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            gen_loss = self.wgan.train_on_batch(noise, valid)\n",
    "            self.gen_loss.append(gen_loss)\n",
    "            \n",
    "            if epoch%100==0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, self.crit_loss[-1], 100*self.crit_acc[-1], self.gen_loss[-1]))\n",
    "                #print (crit_loss_real[0], crit_loss_fake[0], gen_loss)\n",
    "                #print(gen_obs[1,:], np.sum(gen_obs, axis=1))\n",
    "\n",
    "\n",
    "    # Sampling helper function for evaluation\n",
    "    def sampler(self):\n",
    "        z_sample = np.random.normal(0., 1.0, size=(self.n_samples, self.latent_dim))\n",
    "        prediction = self.generator.predict(z_sample).transpose()\n",
    "        samples = np.zeros((self.input_dim_crit, self.n_samples))\n",
    "        samples[:self.numerical_col_n,:]=prediction[:self.numerical_col_n,:]\n",
    "        for idx in range(len(self.categories_cum)-1):\n",
    "            idx_i = self.numerical_col_n+self.categories_cum[idx] # Initial index\n",
    "            idx_f = self.numerical_col_n+self.categories_cum[idx+1] # Final index\n",
    "            mask = np.argmax(prediction[idx_i:idx_f, :], axis=0) + idx_i\n",
    "            for n in range(self.n_samples):\n",
    "                samples[mask[n], n] = 1\n",
    "        return samples\n",
    "    \n",
    "    # VAE evaluation\n",
    "    def wgangp_evaluate(self, used_metric='MAE'):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        # Fit the model\n",
    "        self.wgangp_fit()\n",
    "        \n",
    "        # Evaluate it\n",
    "        self.samples = self.sampler()\n",
    "        self.wgangp_df = samples_to_df(self.samples, print_duplicates=False)\n",
    "        test_df = samples_to_df(self.data_test.transpose(), print_duplicates=False)\n",
    "\n",
    "        # Numerical bin creator \n",
    "        for var in numerical:\n",
    "            test_df[var], bins = pd.cut(test_df[var], bins=5,  retbins=True)\n",
    "            self.wgangp_df[var] = pd.cut(self.wgangp_df[var], bins=bins)\n",
    "\n",
    "        agg_vars = categorical # Variables we are using to aggregate and evaluate, change as needed \n",
    "        ##### Count creator\n",
    "        self.wgangp_df['count'] = 1\n",
    "        self.wgangp_df = self.wgangp_df.groupby(agg_vars, observed=True).count()\n",
    "        self.wgangp_df /= self.wgangp_df['count'].sum()\n",
    "\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby(agg_vars, observed=True).count()\n",
    "        test_df /= test_df['count'].sum()\n",
    "\n",
    "        ##### Merge and difference\n",
    "        real_and_sampled = pd.merge(test_df, self.wgangp_df, suffixes=['_real', '_sampled'], on=categorical, how='outer') # on= all variables\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "        real_and_sampled['diff'] = real_and_sampled.count_real-real_and_sampled.count_sampled\n",
    "        diff = np.array(real_and_sampled['diff'])\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['MAE'] = np.mean(abs(diff))\n",
    "        metrics['MSE'] = np.mean(diff**2)\n",
    "        metrics['RMSE'] = np.sqrt(np.mean(diff**2))\n",
    "        print('Evaluating with {}'.format(used_metric))\n",
    "        print('MAE:{}, MSE:{}, RMSE:{}'.format(metrics['MAE'], metrics['MSE'], metrics['RMSE']))  \n",
    "        \n",
    "        return metrics[used_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_WGANGP = 5000\n",
    "wgangp_latent_dim = 100\n",
    "prueba_WGANGP = WGANGP(train=x_train, test=x_test, numerical_col_n=numerical_col_n,\n",
    "             categorical_col_n = categorical_col_n, categories_n = categories_n, \n",
    "             categories_cum = categories_cum, # Data\n",
    "             intermediate_dim_gen=1024, latent_dim=wgangp_latent_dim, n_hidden_layers_gen=4, # Generator architecture \n",
    "             intermediate_dim_crit=1024, n_hidden_layers_crit=5, # Critic architecture \n",
    "             batch_size=64, epochs=epochs_WGANGP, gen_learn_rate=9e-05, \n",
    "             crit_learn_rate=6e-05, lmda=10, nCritic=5) # Training \n",
    "prueba_WGANGP.wgangp_evaluate()\n",
    "# gen_learn_rate  \n",
    "# crit_learn_rate\n",
    "# latent_dim \n",
    "# n_hidden_layers_gen \n",
    "# n_hidden_layers_crit \n",
    "# intermediate_dim_gen \n",
    "# intermediate_dim_crit \n",
    "# batch_size \n",
    "\n",
    "# The best so far: [1e-05, 1e-02, 2.00e+01, 4.00e+00, 3.00e+00, 5.12e+02, 1.28e+02, 8.00e+00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN main (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(): # Training based on https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "    \n",
    "    def __init__(self, train, test, numerical_col_n, categorical_col_n, categories_n, categories_cum, # Data\n",
    "                     intermediate_dim_gen=256, latent_dim=100, n_hidden_layers_gen=4, # Architecture Generator\n",
    "                     intermediate_dim_dis=128, n_hidden_layers_dis=2, # Architecture Generator\n",
    "                     batch_size=64, epochs=50, validation_split=0.2, gen_learn_rate=0.001, dis_learn_rate=0.001): # Training\n",
    "        \n",
    "        # Data parameters\n",
    "        self.data_train = train\n",
    "        self.data_test = test\n",
    "        \n",
    "        self.numerical_col_n = numerical_col_n # Scalar\n",
    "        self.categorical_col_n = categorical_col_n # Scalar\n",
    "        self.categories_n = categories_n # List of scalars\n",
    "        self.categories_cum = categories_cum # List of scalars\n",
    "        \n",
    "        # Architecture parameters\n",
    "        self.input_dim_dis = train.shape[1]\n",
    "        self.latent_dim = latent_dim # Input dimension for generator\n",
    "        self.intermediate_dim_dis = intermediate_dim_dis\n",
    "        self.intermediate_dim_gen = intermediate_dim_gen\n",
    "        self.n_hidden_layers_dis = n_hidden_layers_dis\n",
    "        self.n_hidden_layers_gen = n_hidden_layers_gen\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.dis_learn_rate = dis_learn_rate\n",
    "        self.gen_learn_rate = gen_learn_rate\n",
    "        self.dis_opt = keras.optimizers.Adam(lr=self.dis_learn_rate)\n",
    "        self.gen_opt = keras.optimizers.Adam(lr=self.gen_learn_rate)\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        \n",
    "        # Sampling parameters\n",
    "        self.n_samples = test.shape[0]\n",
    "        \n",
    "        # Model creation\n",
    "        self.create_gan()\n",
    "        \n",
    "        # Session variables\n",
    "        #tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession() # Start tf session so we can run code.\n",
    "        K.set_session(sess) # Connect keras to the created session.\n",
    "        \n",
    "\n",
    "    # Generator architecture\n",
    "    def create_generator(self):\n",
    "        '''\n",
    "        This is the generator architecture, \n",
    "\n",
    "        params: \n",
    "        n_hidden_layers_gen: number of hidden layers\n",
    "        intermediate_dim_gen: value of the number of neurons for the first intermediate layer. \n",
    "            They increase in a factor of 2 (N*2). \n",
    "        latent_dim: dimension of latent space taken as input for the generator\n",
    "        \n",
    "        output:\n",
    "        The function outputs the generator model in keras. The architecture is defined by the user when\n",
    "        the whole GAN class is initialized.\n",
    "        '''\n",
    "            \n",
    "        gen_input = Input(shape=(self.latent_dim,), name='gen_input')\n",
    "        \n",
    "        # Make the generator intermediate dimensions as it should be first\n",
    "        self.intermediate_dim_gen = int(self.intermediate_dim_gen/2**(self.n_hidden_layers_gen-1))\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_gen):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(gen_input)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim_gen, name= 'generator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(intermediate)\n",
    "            self.intermediate_dim_gen *= 2 # Update the value of the number of neurons\n",
    "\n",
    "        # Final layer\n",
    "        # Categorical decode\n",
    "        x_decoded_mean_cat = [Dense(categories_n[cat], activation='softmax')(intermediate) \n",
    "                              for cat in range(len(self.categories_n))]\n",
    "\n",
    "        if numerical_col_n > 0: # If there are numerical variables, concatenate both\n",
    "            x_decoded_mean_num = Dense(self.numerical_col_n)(intermediate) # Numerical decode\n",
    "            gen_output = concatenate([x_decoded_mean_num] + x_decoded_mean_cat, name='gen_output')\n",
    "        else: # If there are no numerical variables only include the categorical output layer\n",
    "            gen_output = concatenate(x_decoded_mean_cat, name='gen_output')\n",
    "\n",
    "        return Model(inputs=gen_input, outputs=gen_output)\n",
    "    \n",
    "    # Discriminator architecture\n",
    "    def create_discriminator(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "\n",
    "        dis_input = Input(shape=(self.input_dim_dis,), name='dis_input')\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers_dis):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim_dis, name= 'discriminator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(dis_input)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim_dis, name= 'discriminator_hidden_{}'.format(_), kernel_initializer='he_uniform', activation='relu')(intermediate)\n",
    "            self.intermediate_dim = int(self.intermediate_dim_dis/2) # Update the value of the number of neurons\n",
    "        \n",
    "        dis_output = Dense(1, activation='sigmoid', name='dis_output')(intermediate)\n",
    "        \n",
    "        return Model(dis_input, dis_output)\n",
    "    \n",
    "    # GAN creation\n",
    "    def create_gan(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.create_discriminator()\n",
    "        self.discriminator.compile(loss=self.loss, optimizer=self.dis_opt, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.create_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates observations\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated observations as input and discriminates\n",
    "        guess = self.discriminator(generated)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.gan = Model(z, guess)\n",
    "        self.gan.compile(loss=self.loss, optimizer=self.gen_opt) \n",
    "        \n",
    "\n",
    "    def gan_fit(self):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        # Save the generator and discriminator losses and accuracies\n",
    "        self.gen_loss = []\n",
    "        self.dis_loss = []\n",
    "        self.dis_acc = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of observations\n",
    "            idx = np.random.choice(self.data_train.shape[1], self.batch_size, replace=False)\n",
    "            obs = self.data_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_obs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            dis_loss_real = self.discriminator.train_on_batch(obs, valid)\n",
    "            dis_loss_fake = self.discriminator.train_on_batch(gen_obs, fake)\n",
    "            self.dis_loss.append(0.5 * np.add(dis_loss_real[0], dis_loss_fake[0]))\n",
    "            self.dis_acc.append(0.5 * np.add(dis_loss_real[1], dis_loss_fake[1]))\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            self.gen_loss.append(self.gan.train_on_batch(noise, valid))\n",
    "            \n",
    "            if epoch%100==0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, self.dis_loss[-1], 100*self.dis_acc[-1], self.gen_loss[-1]))\n",
    "                #print (dis_loss_real[0], dis_loss_fake[0], gen_loss)\n",
    "                #print(gen_obs[1,:], np.sum(gen_obs, axis=1))\n",
    "\n",
    "    # Sampling helper function for evaluation\n",
    "    def sampler(self):\n",
    "        z_sample = np.random.normal(0., 1.0, size=(self.n_samples, self.latent_dim))\n",
    "        prediction = self.generator.predict(z_sample).transpose()\n",
    "        samples = np.zeros((self.input_dim_dis, self.n_samples))\n",
    "        samples[:self.numerical_col_n,:]=prediction[:self.numerical_col_n,:]\n",
    "        for idx in range(len(self.categories_cum)-1):\n",
    "            idx_i = self.numerical_col_n+self.categories_cum[idx] # Initial index\n",
    "            idx_f = self.numerical_col_n+self.categories_cum[idx+1] # Final index\n",
    "            mask = np.argmax(prediction[idx_i:idx_f, :], axis=0) + idx_i\n",
    "            for n in range(self.n_samples):\n",
    "                samples[mask[n], n] = 1\n",
    "        return samples\n",
    "    \n",
    "    # GAN evaluation\n",
    "    def gan_evaluate(self, used_metric='MAE'):\n",
    "        '''\n",
    "        COMMENT THE CODE\n",
    "        '''\n",
    "        # Fit the model\n",
    "        self.gan_fit()\n",
    "        \n",
    "        # Evaluate it\n",
    "        self.samples = self.sampler()\n",
    "        self.gan_df = samples_to_df(self.samples, print_duplicates=False)\n",
    "        test_df = samples_to_df(self.data_test.transpose(), print_duplicates=False)\n",
    "\n",
    "        # Numerical bin creator \n",
    "        for var in numerical:\n",
    "            test_df[var], bins = pd.cut(test_df[var], bins=5,  retbins=True)\n",
    "            self.gan_df[var] = pd.cut(self.gan_df[var], bins=bins)\n",
    "\n",
    "        agg_vars = categorical # Variables we are using to aggregate and evaluate, change as needed \n",
    "        ##### Count creator\n",
    "        self.gan_df['count'] = 1\n",
    "        self.gan_df = self.gan_df.groupby(agg_vars, observed=True).count()\n",
    "        self.gan_df /= self.gan_df['count'].sum()\n",
    "\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby(agg_vars, observed=True).count()\n",
    "        test_df /= test_df['count'].sum()\n",
    "\n",
    "        ##### Merge and difference\n",
    "        real_and_sampled = pd.merge(test_df, self.gan_df, suffixes=['_real', '_sampled'], on=categorical, how='outer') # on= all variables\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "        real_and_sampled['diff'] = real_and_sampled.count_real-real_and_sampled.count_sampled\n",
    "        diff = np.array(real_and_sampled['diff'])\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['MAE'] = np.mean(abs(diff))\n",
    "        metrics['MSE'] = np.mean(diff**2)\n",
    "        metrics['RMSE'] = np.sqrt(np.mean(diff**2))\n",
    "        print('Evaluating with {}'.format(used_metric))\n",
    "        print('MAE:{}, MSE:{}, RMSE:{}'.format(metrics['MAE'], metrics['MSE'], metrics['RMSE']))  \n",
    "        \n",
    "        return metrics[used_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_GAN = 20000\n",
    "prueba_GAN = GAN(train=x_train, test=x_test, numerical_col_n=numerical_col_n,\n",
    "             categorical_col_n = categorical_col_n, categories_n = categories_n, \n",
    "             categories_cum = categories_cum, # Data\n",
    "             intermediate_dim_gen=1024, latent_dim=100, n_hidden_layers_gen=5, # Generator architecture \n",
    "             intermediate_dim_dis=1024, n_hidden_layers_dis=5, # Discriminator architecture \n",
    "             batch_size=64, epochs=epochs_GAN, gen_learn_rate=0.00001, dis_learn_rate=0.00001) # Training \n",
    "prueba_GAN.gan_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
